<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Luna 的 Bullet Journal 里有些啥]]></title>
    <url>%2F2018%2F04%2F02%2Fbujo-blog%2F</url>
    <content type="text"><![CDATA[2018 年忽然就入坑了手帐，过去的二十年从未有过像今年年初这样重视记录这件事，可能留学的生活确实比较寂寞吧。 入坑前一直觉得做手帐就是在认真装饰一件珍视的东西的同时收获好心情，更多的意义在于消磨时间。实际操作时，我确实有因为写出了比较满意的 Brush letter 而心情变好，买了新的和纸胶带盯着看了半天，认真思考布局排版并将它们实现。总结为以比较低的学习成本享受手工的乐趣。但它无论如何是件分心的事。备考的时候压力山大忽然不想学习就拿手帐出来写写，这个时候手帐就起到了 Brain Dump 的效果：把脑中想完成但暂时完不成的东西安排到今后的日程里，想象出一种写上了就一定会完成的状态，然后在这样愉悦的泡泡中心安理得地发呆…… 很多人做手帐是为了提高个人效率，这个体系理论上是奏效的，但也因人的性格而异吧。不过可以肯定的是， Brain Dump 只少对我来说有缓解焦虑的效果，就算不整整齐齐写在手帐上，写在纸上都有一种大脑被倒空的感觉。 我的第一本手帐是 Leuchtturm1917 的 A5 点阵本。据同样写手帐的室友说选择灯塔本入门实在是少走了许多弯路，因为灯塔的纸质实在太好了，大部分笔在上面不透不洇，纯粹的书写体验。比小妖精 Moleskine 实在多了。拿到手的时候还想着以后用这款就完全够了。（实际上后来又入了各种各样的手账本） 买完本子，下一步考虑手账体系。网上调研一番后，发现手帐的形式实在太多种啦，总的可以归纳为：记录、做计划、拼贴收集。我的话，大概三种都有涉略点，主要倾向于记录，计划只写不看，拼贴看材料和心情。研究了众多博主的手帐体系，最终发现 BUJO 体系最适合我，因为它自由，灵活性强。发展到现在这样的 BUJO 已经和它原来的简约高效注重执行的特点有点相悖，因为形式自由，装饰的风格和空间也自由，玩法一多就不怎么在乎它原来的功能了。当然 Ryder Carroll 设计的 Bullet Journal 还是很实用的，适合所有人。 2018 Setup直至 2018 年 4 月，我一共持有 4 个手账本。这像入坑不到半年的新人吗，我就是管不住这手啊。 Leuchtturm1917 A5 dotted: BUJO, 主日程本 Midori Traveler’s Notebook: 用来写游记/看展览/演出参加活动的记录，主要是收集各种门票啦 Midori 3 years Diary: 日记本，一把年纪了忽然像初中生一样写日记。3/5/10 年日记的想法很好，打算亲自试验一下。每天可以写的位置很小，确保写下来的都是一天中绝对值得记录的重点中的重点而不是废话。 果壳鲸鱼主题笔记本: 因为喜欢鲸鱼所以买了。目前打算用来专门做学业规划。 今年的前三分之一还算比较有时间，几个本子都没有落下。不过接下来硕士入学以后有几本可能就要凉凉，但愿我年底的时候每本都是满的。这篇文章主要还是说一下 BUJO，因为可讲的确实比较多。我个人还是非常喜欢这样能够充分自定义的系统，比起印好日历和布局的日程本，自己在空白本子上规划出来的东西更贴近自己的实际需求。点阵本的好处还在于写字/划线不会歪，也没有辅助线干扰绘图创作。 那么从年初的规划开始说起。 我的规划从 YouTube 上 AmandaRachLee 的年初 BUJO 设置视频中借鉴了很多。//希望有一天也能像这位姐姐画得一样好看 年初就是用来立 Flag 的。 除了 BUJO 体系中必备的目录页，Future Log 页，我还加了： Wishlist //写了就一定买得起 Books To Read //写了就一定会看 Movies To Watch //不写我也会看 Highlights //写了就一定会有 Achievements //留白也是一种美 后两个我一开始真的担心到年底都还是白的，不过今年的前四分之一还是好事比较多。（捧脸 Wishlist 最初的设想是一有闲钱就翻到这页看看。不过按照我的剁手速度好多都是买完了再写上去的…… 回国期间回血了一波之后再也没在上面划掉过任何一项…… 至于书和电影，这两页基本上就是走个形式。豆瓣 8 年用户标过的书影音还缺这两页么。 Monthly按照 BUJO 的规则，每个月都有一个 monthly spread。在这个基础上，喜欢装饰的少女们自然是少不了加个封面画几张图什么的。 封面 我喜欢在封面上加个小日历，只把这个月的好事记下来。 页边的黑色帖标是为了能在合上本子的时候从侧面定位每个月起始的位置。 monthly spread然后是真正的 monthly spread。 比起日历式正方格 monthly overview 我更喜欢这种直排式。理由很明显: //写德语最讨厌换行了！ 能够把事件和备忘区分开，书写空间利用率高很多，整齐美观。 routines很多姑娘会在这个部分加上 habit tracker 来让自己养成好习惯，形式是程序员们再熟悉不过的甘特图哈哈哈。以及 mood tracker，用来绘制自己这一个月的心情变化。moon tracker 非常体现创意，形式上也比 habit tracker 自由多了。特别精致漂亮的 mood tracker 通常还照应这个月的主题。每天填一小格，会有一种戳破 Adventskalender 的兴奋感吗。还有 budget tracker 用来记录自己的收入开支，这个还是用软件记录比较实际点。 这三个模块对我来说都是不必要的，所以也就懒得画。 monthly recap月记录，看心情写。经常控制不住自己的废话写成小说。更大的乐趣在于自己画标题。 Weekly一周一个 weekly spread 再简单不过了。没买 Midori 三年日记本的时候我还会加两页 memo，用来记录无处安放的废话。//曾被室友嘲笑今天天气真好也要写 每天的待办事项是 BUJO 最小的组成单元。BUJO 的核心符号系统定义了玩法：一个实心小圆点代表一个代办事项，完成后划叉。每周/每月回顾一次未完成的事项，如果不打算再做就删除线划掉，如果打算之后再做就划个尖角安排到下个月/周去。如果下个月也不想做，就反方向划尖角写到未来某个月的 future log 里。每个月月初查看本月的 future log 和上个月迁移过来的代办事项，然后开始下一个循环。 当然，也有一整天啥都没干的情况。 weekly recap写周记的我好像小学生啊。 Decoration Ideas花圈还是很好画的w 不想画画还有 mt 胶带呢。 Events &amp; CollectionsBUJO 的自由度又体现在，啥时心血来潮想做个新的主题，完全不用考虑插页的麻烦。灯塔本子每页都有页码，只要在任意一个空白页完成新的主题后将页码索引到本子最前面的目录中就行了。每次在做这一步的时候就特别有种出书的感觉。 这种随意加进日常循环页里的主题页更是想写啥写啥，我写过：影评书评，旅游摘要，聚会回顾，和纸胶带/文具收集，备考面板，相机开光打卡。加了这些页之后 Bujo 虽然按照一定规律重复着格式和布局，但不会变得沉闷，更有人味儿。 作为新手大概就介绍这么多，但愿下半年我也一样这么有闲。]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>BUJO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[千辛万苦等来了 Magisk 对 Pixel(XL) 的官方支持]]></title>
    <url>%2F2017%2F09%2F28%2Fmagisk-for-pixel%2F</url>
    <content type="text"><![CDATA[自从有了折腾手机的念头，我就从未放弃过给自己的 Pixel sailfish 装上 Magisk。毕竟 Xposed 的作者已经以一副要弃坑的姿势搁置这个目前仍相对主流的框架，为了能在抵达欧陆之后玩上小畜生以及其他定制需求, Magisk 成了最吸引人的 systemless root 解决方案。然而 Pixel 的 A/B partition 造成了各种不兼容。众 Pixel(XL) 用户只能依赖非官方支持 (此处感谢 goodwin_c)。这在 Android 8.0 之前还都不是事，Oreo 一出我们就只能巴望着 topjohnwu 的更新（为此我还设置了 twitter 提醒 orz）。 好在 topjohnwu 是个勤劳的开发者，度假回来马上更新了 v14.1，感恩。 目前为止在我的 Pixel sailfish Android 8.0 上没有问题，安装过程中也没出什么幺蛾子。在此记录下安装过程。 DISCLAIMER: 虽然几乎不可能刷砖，但还是得说一句 Do at your own risk. 前期准备 下载 Magisk v14.1 下载 twrp-3.1.1-0-fastboot-sailfish.img (已经刷过 TWRP recovery 的可以忽略这项)。然后重命名为 twrp.img，出于方便键入的考虑。 下载 Magisk Manager v5.3.5 电脑上安装 android-platform-tools (会用到 adb 和 fastboot) 手机已解锁 OEM 手机上设置开启 PIN 或者 Pattern 密码（要不然进入 recovery 之后文件夹会被加密完全找不到想要的文件） 特别提示 如果你之前折腾过，要刷回 Pixel 的原厂 boot.img 或者刷入 Magisk Uninstaller 安装过程 手机链接电脑，将 Magisk-v14.1(1410).zip 和 MagiskManager-v5.3.5.apk 放入存储中 开启 usb 调试 adb devices 确认手机被识别到 adb reboot bootloader 或者关机长按电源+音量-进入 bootloader 模式。 fastboot devices 确认手机被识别到 fastboot boot /path/to/twrp.img 输入密码进入 twrp recovery，选择 install，找到并选中 Magisk-v14.1(1410).zip，swipe 刷入。完成后重启 重启后安装 MagiskManager-v5.3.5.apk 看到此界面说明您已经安装成功 新特性新版本发布前一周 Magisk 作者 topjohnwu 就在 Twitter 上说针对 Pixel A/B 分区会有新特性。那就是，Magisk 和 OTA 更新兼得。无法 OTA 更新可以说是 root 之后最不方便的一件事，现在解决了。 Pixel Exclusive FeaturesSurprise surprise! Thanks to the A/B partition of Pixel devices, we can actually receive and apply OTAs seamlessly using the stock OTA mechanism and preserve Magisk at the same time! 简单说就是在安装 Magisk 的时候它已经备份了原厂 boot 镜像。在需要 OTA 更新时还原到原厂 boot 接收更新。安装完成后再将 Magisk 装回去 Install to Second Slot(After OTA)。重启之后 Magisk 不会被卸载，因为在启动时 Magisk 已经自动给新版本的 boot 打上补丁。 作者也在文档中添加了 OTA 更新相关的指南。亲妈一样的人文关怀。 Magisk 是好东西哎~]]></content>
      <tags>
        <tag>Android</tag>
        <tag>Magisk</tag>
        <tag>Pixel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拼图 in Lunatic Mode]]></title>
    <url>%2F2017%2F09%2F22%2Frosseta-stone-jigsaw-puzzel%2F</url>
    <content type="text"><![CDATA[两个月前我在苏博以八折 150RMB 的价格买下这幅暴虐拼图的时候从没想过有一天能拼完它。 我和对象第一眼看到它的时候都低估了它的难度。只有 800 片，而且包装上的样图看上去很清晰。于是冲动消费买下。 打开包装惊呆。 exo me? 这清晰度和包装上说好的不一样啊。这怎么拼？拼完不瞎了？当场有了转卖给对象的冲动。虽然这么说，旅游结束之后回到家还是马上开工了。 我对拼拼图的经验只有小时候拼的那种 250 片，500 片的儿童拼图，几乎没有难度。按照之前的经验，一般是先搭个框，然后再慢慢从四周往中间补。这是个异形拼图，但异形对难度的影响和图片清晰度比简直不值一提。所以我就麻利地拼好了框 + 左上角的一小块被照亮的部分。 然后？然后这个拼图就这么躺了一个月…… 它不能像常规的拼图一样按颜色初步分类，因为各部分之间的颜色差异实在太小啦。所以我一开始就按照石碑上的三种文字分了类——象形文、埃及文、希腊文。希腊文非常好辨认，而且位于石碑最靠下的三分之一偏暗的位置。埃及文和象形文就很难分开，尤其是象形文比较大，拆成碎片后简直和埃及文不分彼此。就这样勉强分成了三类，分完后就相当于分别完成三幅密银拼图。看不懂碑文所以无法根据文字内容判断附近的碎片是那些。颜色变化起到的帮助微乎其微，只是石碑左边比右边亮一点。那就只好暴力破解呗。 就这样一点一点填。效率最低的时候一小时只拼不到十片。要不是因为临近出国我妈妈嫌我东西占位置，我估计会一直拖延下去。 完成图（血泪 历时两个月，虽然大部分时间都是摊在地上。]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[参观完了上博「大英百物」展]]></title>
    <url>%2F2017%2F07%2F08%2Fhistoryin100objects%2F</url>
    <content type="text"><![CDATA[上博的这个特展从定下日期开始就进入了我的毕业旅行计划。选择七月的第一个月去人潮汹涌的上海可以说是很不明智的，为了掩盖打飞的看展的事实（不是第一次干这事）我们还在前一天去了迪士尼，无可避免地成为了暑期人流的一部分。经历了两天排队，短期内已经不想看到人群了。 开始关注这个展是因为博物志播客 #65 国博策展人的访谈，得知暑假会巡展到上海，满心期待。翻出14年购买的套装书《大英博物馆世界简史》又看了一遍。观展当日非常庆幸自己出发前先做了功课，省下和人群挤着看说明牌的力气。 开展第一天就被新闻中排队的图片吓到差点怂了。经历了开展后第一个周末高峰期后上博增派工作人员并设置了参观特展的单独入口（南门）以应对汹涌人潮。我们是 7 月 4 日去的，也就是开展后的第7天，依然需要痛苦地排队，但是没有前几天排 3 小时那么恐怖了。早上 8 点 35 左右到达上博，坐地铁至大世界站下车走一段路就到。排队人群大多是放了暑假的大中小学生，也有老爷爷老奶奶带着孙子来看展（觉得特别温馨）。9 点开始放人后队伍就开始缓慢移动，排到大概 9 点 40 进馆安检，进了博物馆之后仍然要排 20 分钟左右的队，开始参观的时间大概是 10 点出头。总计排队时间大概一个半小时，这么一想感觉大清早六点多来排队的第一批进馆的人好亏，生生等3小时。 大英百物特展馆门口 进馆前可以去服务台领一本导览册，一边排队一边看，里面有几个重要展品和展线的介绍。 大英博物馆百物展导览册 展馆不大，文物数量又多，稍显拥挤。刚进门的木乃伊棺附近就停留着一堆人，小件展品前面的人群更是移动缓慢。一些在书上见过的展品我就直接不看说明牌了。比如「奥杜威手斧」，实物真的非常精致。书中插图只展示了文物的正面，不看完整个章节真的会觉得不起眼。展览中可以很好地观察它的侧面和背面，想象它的用途，然后就可以说服自己这是石器时代的瑞士军刀了。同样出土于奥杜威的手斧具有圆润的底部，可以想象那是手持的部位。「大洪水记录板」比我想象的要迷你，大概只有成年人巴掌那么大，博物馆商店里还有售卖它的等比例摆件。参观到一半才发现，展品不是原封不动地出自同系列套装书中的展品，而是出于运输和维护的考虑选择了经得起飘洋过海折腾的同样具有代表性的 100 件。想要一睹真容的「莱因德纸草书」、「莫尔德黄金披肩」也意料之内地没有见到。但是看到了「奥尔梅克面具」已经不枉此行。从书中一眼认出是 JOJO 第二部中石假面原型时就激动得嗷嗷叫。(想戴上…) 但是作为挂饰的石假面实际上只有婴儿的脸那么大，我还是停着看了很久。 一些大件非常受欢迎，比如「密特拉屠牛像」、「拉美西斯二世雕像」、「迪蒂摩斯墓碑」这些，可以更近地看到纹理。惊喜的是书中没有收录的「阿兹特克恶灵像」快要萌哭我了，看完解说后也一阵唏嘘，中美洲的民俗真的相当有趣。参观到「湿婆和雪山神女像」的时候是铺面而来满溢的放浪不羁，想象虔诚的信徒在他们面前摆水果的样子。为此我回去后特意翻了书，摘出这样一句： 神也许不应该被设定为孤单的个体，而该是一堆恩爱的快乐夫妻。肉体欢愉不是人类的堕落，而是神性不可或缺的一部分。 或许这也是印度教「包容性」的一部分奥义。 一些展品旁边有个显示屏来播放解说视频，大部分人都不会在这里驻足，而是依赖解说器或是阅读展板。展线迂回，像走迷宫一样有探寻的乐趣，但是部分通道仍然造成了拥塞。参观者素质都蛮高的，不像之前南博、苏博经常有儿童吵闹和大声说话的声音在展馆里回荡。但因为人实在太多了，工作人员需要不停地重复「请不要拍照」、「看完展品请继续向前走」来维持秩序，所以馆内也不是特别的安静。展馆入口处就有禁止拍照的告示牌，然而整个参观过程中大部分人都拿出手机拍照了。也有遵守规定的人用笔和本子记录。整个参展过程还算有序。 上博选出的 101 件展品是二维码，这事我在出发前就得知并大跌眼镜。展馆内用来陈列二维码及介绍的空间大概有一小间房间那么大，一大幅二维码存在感超强让人无处躲藏。走近看这个二维码还是由展品图片拼成的，但是扫不出来。（告诉我扫不出来的为什么能叫二维码？）于是走出展馆，参观完毕，已经接近 12 点。 来之前就对这个展的周边充满期待，但事实上能让我冲动消费的纪念品几乎没有。倒是之前在苏博冲动消费了一套也是大英出品的罗塞塔石碑拼图，虽然鬼畜但诚意满满。我现在就在努力拼它。 几个摆件确实既可爱又精致，帆布包雨伞丝巾什么的没有兴趣，于是最终我买了送给小朋友的绘本和萌萌哒曲奇饼干。（不幸的是我们在机场等延误飞机的时候就把它吃掉了…… 埃及的猫女神芭丝特和丰饶女神伊希丝 总而言之，大英百物展不坑，值得排队。大部分看腻了青铜器和各类陶陶罐罐的参观者都非常喜欢这种 exotic 的体验。 而且整个 A History of the World in 100 Objects 项目主要想展示的就是探寻不同文明的发展之间的微妙联系，如果有心看展的话可以感觉到它想传达的信息。唯一不好的就是人太多了，所以有条件还是错峰观展吧。]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>museum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读: Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition]]></title>
    <url>%2F2016%2F07%2F03%2Flstm%2F</url>
    <content type="text"><![CDATA[LSTM 主要解决的问题Vanishing Gradient Problem The main difficulty lies in the well-known vanishing gradient problem which means that the gradiant that is propagated back through the network either decays or grows expontentially. 举个例子 “The man who wore a wig on his head went inside.” 这句话的主语 man 和谓语 went 之间插入了一个定语从句 who wore a wig on his head，使得两个关键词之间相隔非常远。传统 RNN 不能够捕获这种长距离词汇间的联系，当解析到 went 的时候，主语 man 可能已经被遗忘了，因为 BPTT 通常限制在若干有限步长内。根据链式法则，梯度可以这样计算 $$ \frac{\partial E_3}{\partial W} = \sum_{k=0}^{3}\frac{\partial E_3}{\partial \hat{y}_3}\frac{\partial \hat{y}_3}{\partial s_3} \left(\sum_{j=k+1}^{3}\frac{\partial s_j}{\partial s_{j-1}}\right)\frac{\partial s_k}{\partial W} $$ tanh 函数的导数在接近 $3$ 和 $-3$ 时趋近于 $0$，在多次矩阵乘法中能使远距离时间步的梯度迅速收缩为 $0$， 意味着几步以前的词对当前状态几乎没有贡献。 一个解决方法是使用 ReLU) 取代 sigmoid 或 tanh. 因为 ReLU 的导数要么是 0 要么是 1. 更常见的解决方法是使用 LSTM(Long Short-Term Memory) 或 GRU(Gated Recurrent Unit) 来优化 RNN 中隐层的计算。 LSTM(Long Short-Term Memory)LSTM 本质上只是改进了 RNN 架构中隐层的计算。如果把 LSTM 的隐层当成一个黑盒的话，那么它和 RNN 并没有什么区别。LSTM 的主要特点是它的门机制 (gate mechanism)： input gate, forget gate, output gate. 这三个门进行同样的运算，都是通过 sigmoid 函数将向量的值压缩为 0 或 1. 0 表示丢弃输入值，1 表示让这个输入值通过门并参与到后续运算中。然后和作为状态值的向量(也就是下文要提到的 cell state)逐元素相乘来决定是否让输入向量对状态值作出更新。 lstm_gate Gate 含义 input gate 新计算出来的状态向量有多少要加入后续运算中 forget gate 过滤先前计算出的状态值，即选择要遗忘哪些信息 ouput gate 有多少内部状态信息要暴露给外层神经网络（更高层神经网络或是下一个时间步） 如果把 input gate 的输出全部设为 1， forget gate 的输出全部设为 0 ， output gate 的输出全部设为 1， 那么这就是个普通的 RNN。而这三个门的作用就是为了控制隐层中有多少信息需要保留，有点类似于遮罩. colah 的博客逐步描述了 LSTM 的机制，文章中还有非常直观的可视化。 LSTM 主要由下面五个激活函数组成： $$i_t = \sigma(W_{ix} x_t + W_{im} m_{t-1}+W_{ic} c_{t-1} + b_i)$$$$f_t = \sigma(W_{fx} x_t + W_{mf} m_{t-1} + W_{cf} c_{t-1} + b_f)$$$$c_t = f_t \odot c_{t-1} + i_t \odot g(W_{cx}x_t + W_{ch}h_{t-1} + b_c)$$$$o_t = \sigma(W_{ox}x_t + W_{oh}h_{t-1}+W_{oc}c_t + b_o)$$$$h_t = o_t \odot h(c_t)$$$$y_t = W_{yh}h_t + b_y$$ 符号 含义 W 权重矩阵，比如 $W_{ix}$ 表示 input gate 到输入向量的权重，以此类推 b 偏差向量（bias vector），比如 $b_i$ 是 input gate bias vector $\sigma$ logistic sigmoid 函数 i input gate f forget gate o output gate c 激活向量(cell activation vector) h 输出激活向量（cell ouput activatioin vector) $\odot$ 矩阵逐元素相乘 (elementwise product) Cell Statelstm_cellstate LSTM 的状态参数是为每个隐层节点 (memory cell) 所共享的，换言之就是每个 memory cell 对整个反应链的状态作出修改。colah 将这种 cell state 更新机制类比为传送带。cell state 经过隐层中每个时间步上 memory cell 的更新，而所谓更新就是一些线性运算。 Forget Gatelstm_forgetgate forget gate 的输入包含： 当前时间步的输入向量，上一个时间步中 ouput gate 的输出向量。通过一次 sigmoid 变换映射到 0,1. $f_t$ 决定了旧的状态信息中有多少要遗弃，所以直接 cell state 进行逐元素相乘。 Input Gatelstm_inputgate 这个步骤中 LSTM 做的是把新的信息加入 cell state 中，这项工作包含两部分： input gate layer 和 tanh layer. Input gate 层将决定 cell state 中的哪些值需要被更新，然后 tanh 层生成一组候选值 $\tilde{C}_t$ 来取代 cell state 中需要被更新的旧值. 然后将两个向量逐元素相乘，再与 cell state 相加。input gate 的输出向量也是只有 0 和 1，逐元素相乘后只有为 1 的那些状态量会被更新，所以 $i_t$ 起到的其实是一个遮罩的作用。 Ouput Gatelstm_outputgate 输出仍然是基于当前时间步的 cell state， 这部分同样由一个 sigmoid layer 和 tanh layer 组成。sigmoid 层决定哪些状态信息要被输出，tanh 层将当前 cell state 压缩到 $(-1,1)$ 区间内。然后执行逐元素相乘，产生这个单元的输出变量。输出变量同时作为下一个单元的 $h_t-1$ 加入到循环中。 References | 参考资料Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech RecognitionUnderstanding LSTM Networks – colah’s blogWILDML: Backpropagation Through Time and Vanishing Gradients]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>lstm</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction to RNNs]]></title>
    <url>%2F2016%2F06%2F27%2Frnn%2F</url>
    <content type="text"><![CDATA[Abstract | 简介Language Model | 语言模型语言模型主要解决两件事 能够对计算机生成的句子进行打分，分数高低取决于这个句子有多大概率在现实中出现。分数作为一种语法和语义上的度量 能够产生新的句子。比如用莎士比亚作品作为语料库，可以产生莎士比亚文风的句子。 Recurrent Nerual Network | 循环神经网络关于统计语言模型， Bengio 曾提出过一个使用定长文本训练的前馈神经网络。但这个模型的主要问题是前馈网络依赖于一个预设的文本长度，不能针对不同语境作出动态变化。循环神经网络针对这点作出改进：RNN 不限制文本的长度，而且文本信息能够在网络中循环任意时间。 RNN 的主要思想是充分利用序列化信息之间时间上的关联。传统神经网络将所有输入信息作为相互独立的信息来看待，但在很多情况下（尤其是自然语言处理的问题上）并不是个好的想法。因为语境信息对于语言模型是至关重要的，一个词在某个位置出现的概率很大程度上受前文中其他词的影响。RNN 之所以叫循环神经网络是因为它对序列中每个词的处理是基于前面词的状态。可以理解为 RNN 是一个带有记忆的模型，它能够缓存之前的运算结果，这些状态结果会一直影响接下来的神经元运算。理论上 RNN 能够充分利用任意长度序列的信息，但出于对效率因素的考虑只回看若干步。 trask 博客中的这个 gif 图片能够很直观得看出 RNN 的运行过程。 RNN_GIF 图中展示了 4 个时间步 (timestep) 中隐层 (hidden layer) 的变化。第一个时间步中隐层的计算完全依赖于输入；第二个时间步中隐层的输入来源于第一个隐层的输出和第二个时间步的输入。到了第四个时间步，隐层已经包含了所有输入信息。到第五个时间步，隐层就要决定哪些信息要留下，哪些信息要被覆盖。因此隐层节点越多就能记住更长的时间段内的信息，隐层节点数量意味着循环神经网络的记忆容量。 这样的架构中，输入不仅仅是单纯的输入，输入信息不停地更新 RNN 的 “memory”， 输出是基于某个状态下的 “memory”. 即便某个时间步内没有任何输入, memory 也会随着时间动态发生变化。 先通过一个最基础的 RNN 了解一下它的架构。 Simple recurrent neural network (or Elman Network） 符号表示 含义 $x(t)$ t 时间步的输入信息 $y(t)$ t 时间步的输出信息 $s(t)$ t 时间步的隐层（神经网络的状态） $U$ 隐层和隐层之间的传导矩阵 synapse_h $V$ 隐层和输出层之间的传导矩阵 synapse_1 $f(z)$ sigmoid 激活函数 $g(z_m)$ softmax 函数 $$ x(t) = w(t) + s(t-1) $$ $$ s_j(t) = f\left(\sum_i x_i(t)u_ij \right)$$ $$ y_k(t) = g\left(\sum_j s_j(t)v_kj \right)$$ $$ s.t. f(z) = \frac{1}{1+e^{-z}} , g(z_m) = \frac{e^{z_m}}{\sum_{k}e^{z_k}}$$ $s(0)$ 被初始化为值很小的向量，比如 0.1 。处理大量数据时初始化不是关键因素。隐层节点数量大约在 30-500 之间，反应了训练数据的数量。 Training神经网络大约要经过 10-20 个 epoch 才能达到比较好的收敛效果。BP 算法和 SGD(Stochastic Gradient Descent) 是比较常见的训练方法。设置 learning rate 为 0.1, 每完成一个 epoch 就要在验证集上进行测试，通过观察验证集的对数似然来判断模型是否收敛。如果模型没有显著提高，就将 learning rate 减半，接着进行下一个 epoch . Backpropagation Through Time (BPTT)gif 动态图来自trask 博客 bptt BPTT 是 BP 算法针对循环神经网络的一个改进版本。BPTT 的参数被所有时间步共享。每个输出的梯度依赖于之前的时间步，其实就是对应微积分中的链式法则。例如为了计算 $t = 4$ 时的梯度，需要反向传播 3 次更新权重。BPTT 算法仍有不能解决的问题，比如词汇间的长期依赖(long-term dependency). 两个词隔得很远，但它们之间确有联系。在时间步到达第二个词的时候，第一个词已经被遗忘了。这就是所谓的梯度消失/梯度爆炸 (vanishing/exploding gradient) 问题。针对这个问题已经有了好的解决方法，如 LSTM. LSTM 的架构原则上和一般 RNN 没有什么不同，但在隐层的计算上用了一些技巧，能够选择哪些重要的信息值得记忆。对于捕获词汇间的长期依赖有很大的贡献。 Reference | 参考资料Mikolov: Recurrent neural network based language modelMikolov: Extensions of recurrent neural network language modelWildML:Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNstrask: Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Leaning</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读: Distributed Representation of Words and Phrases and their Compositionality]]></title>
    <url>%2F2016%2F06%2F23%2Fdrwpc%2F</url>
    <content type="text"><![CDATA[简介这篇文章是 Word2Vec 作者关于 Mikolov 的 Skip-gram 模型提出的改进和扩展，主要是提升训练速度和词向量的质量。 关于 Skip-gramSkip-gram 能够高效地从大量文本中训练出词向量。与其他神经网络架构模型不同的是，它不涉及矩阵乘法，大大提升了训练效率。经过学习后的词向量甚至能够进行线性运算。比如： vec(“Madrid”) - vec(“Spain”) + vec(“France”) is closer to vec(“Paris”) skipgram Skip-gram 的训练目标是能够预测文本中某个词周围可能出现的词。比如，现在有一份文档（去掉标点符号）由 $T$ 个词组成，$w_1,w_2,w_3,…,w_T$， skip-gram 的目标函数就是最大化它们的平均对数概率 $$ \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c} \log p(w_{t+j} | w_t) $$ c 是上下文长度，也就是中心词 $w_t$ 的前后各 c 个词， c 越大意味着训练样本越多，精度也就越高，但时间开销大。以 softmax 的方式计算 $p(w_O|w_I)$ $$ p(w_O|w_I) = \frac{\exp({v’_{w_O}}^\top v_{w_I})}{\sum_{w=1}^{W} \exp({v’_{w}}^\top v\_{w_I})} $$ $W$ 是总词汇量，$v_w$ 和 $v’_w$ 是词 $w$ 的输入和输出变量。然而这个公式是不现实的，因为计算开销随着总词汇量 $W$ 而增长。 基于 Skip-gram 的扩展Hierarchical Softmaxskip-gram 中需要 $W$ 个输出节点来计算概率分布，而 Hierarchical Softmax 只需要大约 $\log_2(W)$ 个输出节点，因为它的输出层采用了二叉树的表现形式。每个节点是一个词，它的子节点存储是与这个词相关的词。节点中只存储该词与子节点词的相对概率。总词汇中的每个词都有一条从二叉树根部到自身的路径。用 $n(w,j)$ 来表示从根节点到 w 词这条路径上的第 j 个节点。用 $ch(n)$ 来表示 $n$ 的任意一个子节点。 那么 Hierarchical Softmax 可以表示为 $$ p(w | w_I) = \prod_{j=1}^{L(w)-1} \sigma ([[ n(w,j+1) = ch(n(w,j))]]\cdot {v’_{n(w,j)}}^\top v_{w_I}) $$ $$ s.t. \sigma (x)= \frac{1}{1+\exp(-x)} $$ $ \sum_{w=1}^{W}p(w|w_I) = 1 $ 是可以验证的，这样一来 $ \log p(w_O|w_I)$ 和 $\nabla \log p(w_O|w_I)$ 的计算开销就和 $L(W_O)$ 成比例。而 $L(W_O)$ 不会超过 $\log W$. Word2Vec 中使用的是哈夫曼二叉树，高频词汇编码较短，低频词汇编码较长。 Negative SamplingNCE(Noise Contrastive Estimation):好的模型能够通过逻辑斯蒂回归将数据从噪声中区别出来 因为 skip-gram 更关注于学习高质量的词向量表达，所以可以在保证词向量质量的前提下对 NCE 进行简化。于是定义了 NEG(Negative Sampling) $$ \log \sigma({v’_{w_O}}^\top v_{w_I} )+ \sum_{i=1}^{k}\mathbb{E}_{w_i \sim P_n(w)}[\log \sigma(-{v’_{w_i}}^\top v_{w_I})] $$ 这个公式用来替代 skip-gram 目标函数中的 $\log p(w_O|w_I)$. $ P_n(w)$ 是词 $n$ 周围的噪声词分布（NCE 和 NEG 都有这个参数）。这个公式是用逻辑斯蒂回归从 $k$ 个负例中区别出目标词汇。对于小的训练集 $k$ 的最佳取值是 5-20， 对于大的训练集 $k$ 的取值会更小，大概 2-5. NCE 和 NEG 的区别在于 NCE 在计算时需要样本和噪音分布的数值概率，而 NEG 只需要样本。 Subsamplingsubsampling 用于解决这样的问题：在英文中有大量高频词诸如 “in”, “the”, “a”, 这些词提供的信息量非常少。比如 “Paris” 和 “France” 的共现率明显比 “France” 和 “The” 的共现率低，但 “Paris” 和 “France” 明显更有用。 为了解决高频和低频词汇间的不平衡，每个词汇的频率有一定概率被丢弃，概率由公式得出： $$ P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}$$ $ f(w_i)$ 是词 $w_i$ 的概率. $t$ 是参数，作为淘汰词的门限，通常情况下约为 $10^{-5}$. 当词汇的频率高于 $t$ 就有一定概率被淘汰掉。 处理 Phrase这部分主要解决专有名词的问题，比如期刊杂志的名字 “New York Times” 并不是单词 “New York” 和 “Times” 词义的简单组合，所以要把它们当成一个词组来看待。作者用了一种简单的数据驱动方式来识别这些专有名词。对于两个词 $w_i, w_j$： $$ score(w_i, w_j) = \frac{count(w_i w_j) - \delta}{count(w_i) \times count(w_j)}$$ 其中 $\delta$ 是协同系数，用来避免太多无关的词被组合到一起。 资源下载Distributed Representation of Words and Phrases and their Compositionality]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
</search>
